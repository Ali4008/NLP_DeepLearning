{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":986425,"sourceType":"datasetVersion","datasetId":540042},{"sourceId":4873801,"sourceType":"datasetVersion","datasetId":2825963}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:55:40.008449Z","iopub.execute_input":"2025-05-13T00:55:40.008793Z","iopub.status.idle":"2025-05-13T00:55:46.619181Z","shell.execute_reply.started":"2025-05-13T00:55:40.008765Z","shell.execute_reply":"2025-05-13T00:55:46.618446Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q nltk\nimport nltk\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:55:46.620644Z","iopub.execute_input":"2025-05-13T00:55:46.620927Z","iopub.status.idle":"2025-05-13T00:55:51.707702Z","shell.execute_reply.started":"2025-05-13T00:55:46.620905Z","shell.execute_reply":"2025-05-13T00:55:51.707035Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:55:51.708537Z","iopub.execute_input":"2025-05-13T00:55:51.708979Z","iopub.status.idle":"2025-05-13T00:55:59.780380Z","shell.execute_reply.started":"2025-05-13T00:55:51.708957Z","shell.execute_reply":"2025-05-13T00:55:59.779532Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HuggingfaceAPI\")\n\nlogin(token=hf_token)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:55:59.782223Z","iopub.execute_input":"2025-05-13T00:55:59.782652Z","iopub.status.idle":"2025-05-13T00:56:00.914748Z","shell.execute_reply.started":"2025-05-13T00:55:59.782600Z","shell.execute_reply":"2025-05-13T00:56:00.913986Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!pip install -q gensim\n\n# Download fastText Urdu embeddings by Facebook\n!wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ur.300.vec.gz\n!gunzip cc.ur.300.vec.gz\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:56:00.915575Z","iopub.execute_input":"2025-05-13T00:56:00.915906Z","iopub.status.idle":"2025-05-13T00:56:33.884528Z","shell.execute_reply.started":"2025-05-13T00:56:00.915879Z","shell.execute_reply":"2025-05-13T00:56:33.883700Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\n\n# Your loaded text files\nwith open(\"/kaggle/input/parallel-corpus-for-english-urdu-language/Dataset/english-corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n    english_lines = f.read().splitlines()\n\nwith open(\"/kaggle/input/parallel-corpus-for-english-urdu-language/Dataset/urdu-corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n    urdu_lines = f.read().splitlines()\n\n# Filter misaligned lines\npairs = [(u, e) for u, e in zip(urdu_lines, english_lines) if u.strip() and e.strip()]\nurdu_lines, english_lines = zip(*pairs)\n\ndf = pd.DataFrame({\"translation\": [{\"ur\": u, \"en\": e} for u, e in zip(urdu_lines, english_lines)]})\n\n# Split dataset\ntrain_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\ntrain_ds = Dataset.from_pandas(train_df)\nval_ds = Dataset.from_pandas(val_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:56:33.885818Z","iopub.execute_input":"2025-05-13T00:56:33.886708Z","iopub.status.idle":"2025-05-13T00:56:35.259842Z","shell.execute_reply.started":"2025-05-13T00:56:33.886681Z","shell.execute_reply":"2025-05-13T00:56:35.259200Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_ds[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:56:35.260553Z","iopub.execute_input":"2025-05-13T00:56:35.261001Z","iopub.status.idle":"2025-05-13T00:56:35.272402Z","shell.execute_reply.started":"2025-05-13T00:56:35.260981Z","shell.execute_reply":"2025-05-13T00:56:35.271598Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'translation': {'en': 'i was wide awake', 'ur': 'میں وسیع جاگ رہا تھا'},\n '__index_level_0__': 11378}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\n\n# Extract English and Urdu translations\ndata = [{'en': item['translation']['en'], 'ur': item['translation']['ur']} for item in train_ds]\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:56:35.273137Z","iopub.execute_input":"2025-05-13T00:56:35.273323Z","iopub.status.idle":"2025-05-13T00:56:36.045546Z","shell.execute_reply.started":"2025-05-13T00:56:35.273301Z","shell.execute_reply":"2025-05-13T00:56:36.044704Z"}},"outputs":[{"name":"stdout","text":"                     en                    ur\n0      i was wide awake  میں وسیع جاگ رہا تھا\n1  she broke into tears             وہ رو پڑی\n2       we walked a lot         ہم بہت چل پڑے\n3   i am in charge here  میں یہاں انچارج ہوں۔\n4   there is no mistake     کوئی غلطی نہیں ہے\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"df[\"src_tok\"] = df[\"ur\"].apply(word_tokenize)\ndf[\"tgt_tok\"] = df[\"en\"].apply(word_tokenize)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:56:36.046471Z","iopub.execute_input":"2025-05-13T00:56:36.047251Z","iopub.status.idle":"2025-05-13T00:56:38.247506Z","shell.execute_reply.started":"2025-05-13T00:56:36.047231Z","shell.execute_reply":"2025-05-13T00:56:38.246981Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Function to build a word-to-index vocabulary from tokenized sentences\ndef build_vocab(tokenized_texts, min_freq=1):\n    freq = {}  # Dictionary to store word frequencies\n\n    # Count frequency of each word across all tokenized sentences\n    for sent in tokenized_texts:\n        for word in sent:\n            freq[word] = freq.get(word, 0) + 1\n\n    # Initialize vocabulary with special tokens and their reserved indices\n    vocab = {\n        \"<pad>\": 0,  # Padding token\n        \"<sos>\": 1,  # Start of sequence\n        \"<eos>\": 2,  # End of sequence\n        \"<unk>\": 3   # Unknown token (for out-of-vocabulary words)\n    }\n\n    idx = 4  # Starting index for actual words\n\n    # Add words to vocabulary that meet or exceed the minimum frequency threshold\n    for word, count in freq.items():\n        if count >= min_freq:\n            vocab[word] = idx\n            idx += 1\n\n    return vocab  # Return the final vocabulary mapping\n\n# Build vocabulary for source (Urdu) and target (English) tokenized texts\nsrc_vocab = build_vocab(df[\"src_tok\"])  # Urdu word-to-index mapping\ntgt_vocab = build_vocab(df[\"tgt_tok\"])  # English word-to-index mapping\n\n# Create reverse vocabulary for target (index-to-word), useful for decoding predictions\ninv_tgt_vocab = {i: w for w, i in tgt_vocab.items()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:56:38.249219Z","iopub.execute_input":"2025-05-13T00:56:38.249484Z","iopub.status.idle":"2025-05-13T00:56:38.295109Z","shell.execute_reply.started":"2025-05-13T00:56:38.249465Z","shell.execute_reply":"2025-05-13T00:56:38.294518Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Load GloVe\ndef load_glove_embeddings(glove_path, vocab, embedding_dim=100):\n    embeddings_index = {}\n    with open(glove_path, encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = vector\n\n    matrix = np.zeros((len(vocab), embedding_dim))\n    for word, i in vocab.items():\n        vector = embeddings_index.get(word)\n        if vector is not None:\n            matrix[i] = vector\n        else:\n            matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n    return torch.tensor(matrix, dtype=torch.float32)\n\n# Download GloVe manually and update path\nglove_path = \"/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\"  # Ensure it's in working dir\nembedding_dim = 100\ntgt_embedding_matrix = load_glove_embeddings(glove_path, tgt_vocab, embedding_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:56:38.295830Z","iopub.execute_input":"2025-05-13T00:56:38.296062Z","iopub.status.idle":"2025-05-13T00:56:49.027282Z","shell.execute_reply.started":"2025-05-13T00:56:38.296035Z","shell.execute_reply":"2025-05-13T00:56:49.026745Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\n\n# Custom PyTorch dataset for translation tasks\nclass TranslationDataset(Dataset):\n    def __init__(self, df, src_vocab, tgt_vocab, max_len=20):\n        self.df = df                  # DataFrame containing tokenized source and target sentences\n        self.src_vocab = src_vocab    # Vocabulary mapping for source language (e.g., Urdu)\n        self.tgt_vocab = tgt_vocab    # Vocabulary mapping for target language (e.g., English)\n        self.max_len = max_len        # Maximum length for input/output sequences (for padding)\n\n    def __len__(self):\n        # Returns total number of sentence pairs\n        return len(self.df)\n\n    def encode(self, tokens, vocab, add_sos_eos=True):\n        # Ensures that token list isn't nested accidentally\n        if isinstance(tokens[0], list):\n            tokens = tokens[0]\n\n        # Convert tokens to their corresponding indices, using <unk> if word is out of vocab\n        ids = [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokens]\n\n        # Optionally add <sos> and <eos> tokens\n        if add_sos_eos:\n            ids = [vocab[\"<sos>\"]] + ids + [vocab[\"<eos>\"]]\n\n        # Pad or trim sequence to exactly max_len\n        return ids[:self.max_len] + [vocab[\"<pad>\"]] * max(0, self.max_len - len(ids))\n\n    def __getitem__(self, idx):\n        # Get the source and target token lists for the given index\n        src_tokens = self.df.iloc[idx][\"src_tok\"]\n        tgt_tokens = self.df.iloc[idx][\"tgt_tok\"]\n\n        # Handle case where token lists might be nested (e.g., [['word1', 'word2']])\n        if isinstance(src_tokens[0], list):\n            src_tokens = src_tokens[0]\n        if isinstance(tgt_tokens[0], list):\n            tgt_tokens = tgt_tokens[0]\n\n        # Encode token lists into padded sequences of token IDs\n        src = self.encode(src_tokens, self.src_vocab)\n        tgt = self.encode(tgt_tokens, self.tgt_vocab)\n\n        # Return as PyTorch tensors\n        return torch.tensor(src), torch.tensor(tgt)\n\n# Instantiate the dataset with your DataFrame and vocabularies\ntrain_dataset = TranslationDataset(df, src_vocab, tgt_vocab)\n\n# Create a DataLoader to batch and shuffle data during training\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T00:56:49.028051Z","iopub.execute_input":"2025-05-13T00:56:49.028233Z","iopub.status.idle":"2025-05-13T00:56:49.036081Z","shell.execute_reply.started":"2025-05-13T00:56:49.028219Z","shell.execute_reply":"2025-05-13T00:56:49.035362Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\n\n# Positional encoding module (as described in \"Attention Is All You Need\")\nclass PositionalEncoding(nn.Module):\n    def __init__(self, emb_size, dropout=0.1, maxlen=5000):\n        super().__init__()\n\n        # Compute the positional encodings once in log space\n        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)  # Even indices: sine\n        pos_embedding[:, 1::2] = torch.cos(pos * den)  # Odd indices: cosine\n\n        self.dropout = nn.Dropout(dropout)\n        # Save positional encodings as a buffer so they're not trainable\n        self.register_buffer('pos_embedding', pos_embedding.unsqueeze(1))\n\n    def forward(self, token_embedding):\n        # Add positional encodings to token embeddings and apply dropout\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\n\n# Transformer-based seq2seq model with pretrained target embeddings\nclass ImprovedTransformerModel(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, emb_size, tgt_emb_matrix,\n                 nhead=4, num_layers=4, dim_ff=512, dropout=0.1):\n        super().__init__()\n        self.emb_size = emb_size\n\n        # Source token embedding (learnable)\n        self.src_embedding = nn.Embedding(src_vocab_size, emb_size)\n        self.pos_encoder = PositionalEncoding(emb_size, dropout)\n\n        # Target token embedding initialized from GloVe (trainable)\n        self.tgt_embedding = nn.Embedding.from_pretrained(tgt_emb_matrix, freeze=False)\n        self.pos_decoder = PositionalEncoding(emb_size, dropout)\n\n        # PyTorch's built-in Transformer module\n        self.transformer = nn.Transformer(\n            d_model=emb_size,               # Embedding size (must match throughout)\n            nhead=nhead,                    # Number of attention heads\n            num_encoder_layers=num_layers, # Layers in encoder\n            num_decoder_layers=num_layers, # Layers in decoder\n            dim_feedforward=dim_ff,        # Size of feedforward sublayer\n            dropout=dropout,               # Dropout rate\n            batch_first=True               # Use batch-first tensor shape: (batch, seq, dim)\n        )\n\n        # Output layer: maps final decoder output to target vocabulary logits\n        self.fc_out = nn.Linear(emb_size, tgt_vocab_size)\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):\n        # Apply source embedding + positional encoding\n        src_emb = self.pos_encoder(self.src_embedding(src))\n\n        # Apply target embedding (GloVe) + positional encoding\n        tgt_emb = self.pos_decoder(self.tgt_embedding(tgt))\n\n        # Pass through transformer encoder-decoder\n        out = self.transformer(\n            src_emb, tgt_emb,\n            src_mask=src_mask, tgt_mask=tgt_mask,\n            src_key_padding_mask=src_padding_mask,\n            tgt_key_padding_mask=tgt_padding_mask\n        )\n\n        # Project transformer output to vocabulary logits\n        return self.fc_out(out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T01:00:33.212910Z","iopub.execute_input":"2025-05-13T01:00:33.213599Z","iopub.status.idle":"2025-05-13T01:00:33.222402Z","shell.execute_reply.started":"2025-05-13T01:00:33.213566Z","shell.execute_reply":"2025-05-13T01:00:33.221665Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"model = ImprovedTransformerModel(\n    src_vocab_size=len(src_vocab),\n    tgt_vocab_size=len(tgt_vocab),\n    emb_size=embedding_dim,\n    tgt_emb_matrix=tgt_embedding_matrix,\n    nhead=4,\n    num_layers=4,\n    dim_ff=512,\n    dropout=0.1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T01:00:50.784471Z","iopub.execute_input":"2025-05-13T01:00:50.785295Z","iopub.status.idle":"2025-05-13T01:00:51.008240Z","shell.execute_reply.started":"2025-05-13T01:00:50.785265Z","shell.execute_reply":"2025-05-13T01:00:51.007395Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from tqdm import tqdm\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\nfor epoch in range(10):\n    model.train()\n    total_loss = 0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n    for src, tgt in loop:\n        optimizer.zero_grad()\n        output = model(src, tgt[:, :-1])\n        loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n        # Update tqdm description\n        loop.set_postfix(loss=loss.item())\n\n    print(f\"Epoch {epoch+1} | Total Loss: {total_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T01:00:53.739512Z","iopub.execute_input":"2025-05-13T01:00:53.740372Z","iopub.status.idle":"2025-05-13T03:14:53.854904Z","shell.execute_reply.started":"2025-05-13T01:00:53.740340Z","shell.execute_reply":"2025-05-13T03:14:53.854054Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 11036/11036 [10:58<00:00, 16.77it/s, loss=5.29]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Total Loss: 61016.7812\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 11036/11036 [11:26<00:00, 16.08it/s, loss=4.44]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Total Loss: 59837.4122\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 11036/11036 [11:47<00:00, 15.61it/s, loss=5.02]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Total Loss: 59724.6356\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 11036/11036 [12:27<00:00, 14.77it/s, loss=5.22]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Total Loss: 59693.7212\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 11036/11036 [13:24<00:00, 13.72it/s, loss=5.71]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Total Loss: 59681.1133\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 11036/11036 [14:13<00:00, 12.93it/s, loss=5.31]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Total Loss: 59654.8088\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 11036/11036 [14:34<00:00, 12.62it/s, loss=6.09]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Total Loss: 59644.5483\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 11036/11036 [14:54<00:00, 12.33it/s, loss=5.85]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Total Loss: 59654.5586\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 11036/11036 [15:02<00:00, 12.23it/s, loss=5.03]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Total Loss: 59672.2464\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 11036/11036 [15:08<00:00, 12.15it/s, loss=5.29]","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Total Loss: 59689.4144\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}